{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Reinforcement Learning with OpenAI gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy.stats import zscore as z_transform\n",
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, create a new environment! CartPole is a game where a cart moves left or right along a frictionless track to try to balance a pole placed on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is the system in which our AI will learn. We recieve information from the environment by making observations, and influence the environment by taking actions. What do these observations look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
      "[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.shape)\n",
    "print(env.observation_space.high)\n",
    "print(env.observation_space.low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observation is 4 dimensional with the upper and lower bounds show. What are these dimensions? We will try to find this out later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.action_space)\n",
    "print([env.action_space.sample() for _ in range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action space allows us to take two actions: applying a momentary force to the left or right. But which is which? Let's take a look at the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment to start a new session, with the pole near vertical and the cart in the middle.\n",
    "env.reset()\n",
    "for _ in range(30):\n",
    "    env.render() # prints the environment to another window\n",
    "    #observation, reward, done, info = env.step(0)\n",
    "    #observation, reward, done, info = env.step(1)\n",
    "    observation, reward, done, info = env.step(env.action_space.sample()) # choose a random action and send it to the env!\n",
    "    print(observation, reward, done, info)\n",
    "    time.sleep(0.5)\n",
    "env.close() # this is the only way to close the window!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe if you're brighter than me you can figure out exactly what each of these dimensions mean. Luckily, we don't care! Our machine will figure it out. What's important is that we see that we are getting a reward of 1 at each time step that done = False, and after done = True, we get no more reward. This occurs when the angle of the pole is too large, or the cart has drifted too far from the center. This is the end of a \"session\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve the problem of what action to take given an observation, we will use a neural network. For our current purposes, you can think of it as a magical black box that converts an input (4-dimensional observation) to an output (probabilities of taking each action in the 2-dimensional action space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([tf.keras.layers.Input(shape=env.observation_space.shape),\n",
    "                             tf.keras.layers.Dense(4, activation='sigmoid'),\n",
    "                             tf.keras.layers.Dense(env.action_space.n, activation=tf.nn.softmax)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you just made a neural network! The code above uses a high-level interface called keras to easily generate models. This neural network has an input layer that takes in the observations, a hidden middle layer which has magical unknown abilities, and an output layer that corresponds to probabilities of actions. Unfortunately, it is not smart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment to start a new session, with the pole near vertical and the cart in the middle.\n",
    "observation = env.reset()\n",
    "for _ in range(30):\n",
    "    env.render()\n",
    "    # convert the observation into a form that the model can use as an input\n",
    "    action_dist = model.predict(tf.convert_to_tensor(tf.expand_dims(observation, 0)))[0]\n",
    "    # sample from the action space\n",
    "    action = int(np.random.choice(np.arange(len(action_dist)), p=action_dist))\n",
    "    # send that action to the environment to see what happens\n",
    "    observation, reward, done, info = env.step(action) \n",
    "    time.sleep(0.5)\n",
    "env.close() # this is the only way to close the window!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our model (that is, our AI, or our neural network) has been randomly initialized, its behaviour should also be pretty random. Our goal is to have it make educated choices about what action to take given an observation. The first step in this goal is to run a bunch of sessions and see how the model is performing. Then we will know how to adjust its behaviour. Here's what a function to run a session looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    all_obs = []\n",
    "    all_ac = []\n",
    "    rw = 0\n",
    "    observation = env.reset()\n",
    "\n",
    "    while True:\n",
    "        # get the distribution over action space from the model\n",
    "        action_dist = model.predict(tf.convert_to_tensor(tf.expand_dims(observation, 0)))[0]\n",
    "        # sample from the action space\n",
    "        action = int(np.random.choice(np.arange(len(action_dist)), p=action_dist))\n",
    "        # get the information about the state of the system following the action\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        all_obs.append(observation)\n",
    "        all_ac.append(action)\n",
    "        rw += reward\n",
    "        # done specifies that the session is over, usually due to a win or loss\n",
    "        if done:\n",
    "            break\n",
    "    env.close()\n",
    "\n",
    "    # return the observations and rewards (the reward will be discounted later)\n",
    "    return all_obs, all_ac, rw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, we're just collecting information in a bunch of lists to use for training. We have made the decision to count the reward for the session as the sum of rewards recieved throughout the session: the higher the final reward, the longer the pole was balanced. Also, we stop getting rewards when 'done' (when the pole has rotated a certain angle from normal), so we will stop the session at that point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A problem we now encounter is that there is no real association between rewards recieved and specific actions taken; a reward is given based on the collection of all actions taken in a single session, but which actions contributed to success? Here we will make another simplifying assumption: the actions taken closest to the end of the session matter the most. This isn't the case for every problem, but in this case you can imagine that the decisions made when the pole is tipping are the most important. How do we account for this? We make the rewards smaller the further away from the end of the session they occur, 'discounting' them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount(rw, gamma=0.9):\n",
    "    # weight individual rewards with an exponential decay function\n",
    "    # since the magnitude should be largest close to the end of the session, apply the weights in reverse order\n",
    "    weights = np.array([gamma**(rw.shape[0]-i-1) for i in range(rw.shape[0])])\n",
    "    discounted = tf.convert_to_tensor(weights, dtype=tf.float32) * rw\n",
    "    return discounted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, rw is a list of the same length as the number of frames in a corresponding session. These represent the reward given for the actions taken in each frame. We discount the rewards by weighting them with an exponential decay function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do these collected observations, actions, and discounted rewards apply to the model? They are connected by a loss function, which determines how the parameters of the model should change to better react to its environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(obs, ac, rw):\n",
    "    y_pred = model(obs)\n",
    "    y_true = ac\n",
    "    loss = sum((y_true - y_pred)** 2) * rw\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'loss' is a function of an observation, an associated action, and the received reward for that action in that context. y_pred is the probability distribution (over the two actions) that the model predicts, while y_true is the action that it actually chose. y_pred is binomial and sums to 1, something like [0.25, 0.75], while y_true is binomial and discrete, like [0, 1]. If y_pred was [0.25, 0.75], that means the force-right action has a 75% possibility of being sampled, and we see that in the case of [0, 1], it was indeed chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what is the point of this loss function? Well, let's say that we are looking at one single frame of a session, where we think the chosen action led to a high reward, such that rw is positive and large. We want to reinforce the selected action, to make it more probable to occur in the future. So we want y_pred to closer match y_true (increasing its probability), and we tell our model to make it so by changing its parameters to minimize this loss. Ultimately, this is just a fancy optimization task. This loss function is computed for every frame of every section, each with its own action, observation, and discounted reward. That's a lot of data to use to improve!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you think we would want to change the probability distribution (with respect to the chosen action) if the reward was negative?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we are surprisingly close to being finished! The last step is to put everything together in a training loop. This loop will take our existing functions and tie them all together. \n",
    "\n",
    "    -First, we will run a number of sessions and collect all of the data: observations, actions, and rewards.\n",
    "\n",
    "    -Then, we will normalize the rewards. If you have been paying attention, you may have noticed that until now, the \n",
    "    rewards are all positive! We want to punish failure and reward success for stable training. We then discount the rewards \n",
    "    for each section.\n",
    "\n",
    "    -Then, we will compute the loss. This code is vectorized, enabling us to use matrix math to compute the loss for a whole \n",
    "    session all at once! The gradient tape tool will keep track of how all the parameters need to change to reduce the loss. \n",
    "    All we need to do is calculate the loss \"with\" the gradient tape, and this information will be saved.\n",
    "\n",
    "    -Finally, we apply the gradients to our neural network, changing its parameters to minimize the loss, and hopefully, \n",
    "    respond more appropriately to its environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_train(self):\n",
    "    obs_list = []\n",
    "    ac_list = []\n",
    "    rw_list = []\n",
    "\n",
    "    for _ in range(self.batch_size):\n",
    "        obs, ac, rw = self.run()\n",
    "        obs_list.append(obs)\n",
    "        ac_list.append(ac)\n",
    "        rw_list.append(rw)\n",
    "\n",
    "    # normalize rewards. this will cause the worst performing sessions of the batch to recieve negative rewards\n",
    "    # while the best performing ones recieve positive rewards\n",
    "    rw_norm = z_transform(np.array(rw_list))\n",
    "    # convert to tensors to discount. each frame of a session is initially assigned the same reward\n",
    "    # which is the sum of the rewards obtained at each frame of the session\n",
    "    rw_tensors = [tf.ones(shape=len(obs_list[i])) * rw for i, rw in enumerate(rw_norm)]\n",
    "    # list of discounted rewards in session\n",
    "    rw_discount = [discount(rw) for rw in rw_tensors]\n",
    "    # list of observations\n",
    "    obs_tensors = [tf.convert_to_tensor(obs, dtype=tf.float32) for obs in obs_list]\n",
    "    # list of actions\n",
    "    ac_tensors = [tf.one_hot(ac, depth=env.action_space.n) for ac in ac_list]\n",
    "\n",
    "    gradients = []\n",
    "\n",
    "    # for each session of observations and discounted rewards\n",
    "    for obs, act, rw in zip(obs_tensors, ac_tensors, rw_discount):\n",
    "        # compute the gradient of the selected actions with respect to the observations of the environment\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.compute_loss(obs, act, rw)\n",
    "            loss = tf.convert_to_tensor(loss, dtype=tf.float32)\n",
    "        g = tape.gradient(loss, self.model.trainable_variables)\n",
    "        # collect all the gradients, instead of applying them at each step which would give inaccurate rewards\n",
    "        gradients.append(g)\n",
    "    avg_gradients = []\n",
    "\n",
    "    # each k represents a kernel's gradients\n",
    "    # can't do this as a tensor because shapes change\n",
    "    for k in range(len(gradients[0])):\n",
    "        # get all of the gradients associated to one kernel\n",
    "        t = tf.convert_to_tensor([grad[k] for grad in gradients])\n",
    "        t = tf.reduce_mean(t, axis=0)\n",
    "        avg_gradients.append(t)\n",
    "\n",
    "    # apply the gradients to their respective variables\n",
    "    # can't do this until all gradients have been calculated\n",
    "    self.optimizer.apply_gradients(zip(avg_gradients, self.model.trainable_variables))\n",
    "    print(self.model.weights)\n",
    "\n",
    "    return 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
